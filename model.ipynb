{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# 讀取上傳的文件\n",
    "'''\n",
    "north_data = pd.read_csv('D:/DS_Prediction/Weather/north_weekly_averages.csv')\n",
    "south_data = pd.read_csv('D:/DS_Prediction/Weather/south_weekly_averages.csv')\n",
    "central_data = pd.read_csv('D:/DS_Prediction/Weather/central_weekly_averages.csv')\n",
    "east_data = pd.read_csv('D:/DS_Prediction/Weather/east_weekly_averages.csv')\n",
    "fuel_prices = pd.read_csv('D:/DS_Prediction/fuel_prices.csv')\n",
    "cabbage_prices = pd.read_csv('D:/DS_Prediction/Domestic_Cabbage.csv')\n",
    "'''\n",
    "\n",
    "north_data = pd.read_csv('C:/Users/$EKH000-V5FHVTC5DRPM/DS_Prediction/weather-csv/north_weekly_averages.csv')\n",
    "south_data = pd.read_csv('C:/Users/$EKH000-V5FHVTC5DRPM/DS_Prediction/weather-csv/south_weekly_averages.csv')\n",
    "central_data = pd.read_csv('C:/Users/$EKH000-V5FHVTC5DRPM/DS_Prediction/weather-csv/central_weekly_averages.csv')\n",
    "east_data = pd.read_csv('C:/Users/$EKH000-V5FHVTC5DRPM/DS_Prediction/weather-csv/east_weekly_averages.csv')\n",
    "fuel_prices = pd.read_csv('C:/Users/$EKH000-V5FHVTC5DRPM/DS_Prediction/fuel_prices.csv')\n",
    "cabbage_prices = pd.read_csv('C:/Users/$EKH000-V5FHVTC5DRPM/DS_Prediction/vegetable-csv/Domestic_Cabbage.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to Train_X.csv\n",
      "                       date_x    平均價       交易量  year  month  week  平均氣壓(hPa)  \\\n",
      "0   2019-01-01 00:00:00+00:00  22.22   4487.67  2019      1     0    1019.18   \n",
      "1   2019-01-01 00:00:00+00:00  22.22   4487.67  2019      1     0    1019.90   \n",
      "2   2019-01-01 00:00:00+00:00  22.22   4487.67  2019      1     0    1020.22   \n",
      "3   2019-01-01 00:00:00+00:00  22.22   4487.67  2019      1     0    1017.62   \n",
      "4   2019-01-08 00:00:00+00:00  20.40   4323.00  2019      1     1    1016.19   \n",
      "..                        ...    ...       ...   ...    ...   ...        ...   \n",
      "922 2024-11-12 00:00:00+00:00  47.13  14259.17  2024     11     1    1006.90   \n",
      "923 2024-11-25 00:00:00+00:00  40.70  17676.00  2024     11     3    1015.60   \n",
      "924 2024-11-25 00:00:00+00:00  40.70  17676.00  2024     11     3    1014.70   \n",
      "925 2024-11-25 00:00:00+00:00  40.70  17676.00  2024     11     3    1016.55   \n",
      "926 2024-11-25 00:00:00+00:00  40.70  17676.00  2024     11     3    1013.20   \n",
      "\n",
      "     平均氣溫(℃)  平均相對溼度( %)  平均風速(m/s)  累計雨量(mm)  累積日射量(MJ/m2)  \\\n",
      "0      17.33       95.67       3.53      2.50          6.57   \n",
      "1      19.53       89.83       1.40      4.00          6.81   \n",
      "2      19.42       83.00       4.40      0.00         11.32   \n",
      "3      20.00       82.33       1.70      0.08         11.20   \n",
      "4      18.63       93.14       2.61      0.00          7.28   \n",
      "..       ...         ...        ...       ...           ...   \n",
      "922    24.80       91.86       1.27      8.50         10.08   \n",
      "923    18.40       83.00       4.20      1.00          5.10   \n",
      "924    21.15       81.50       1.75      5.75         11.34   \n",
      "925    21.65       76.50       3.65      0.00         15.82   \n",
      "926    22.25       84.00       2.25      0.00         16.58   \n",
      "\n",
      "                       date_y    Fuel_92    Fuel_95  Fuel_High  \n",
      "0                         NaT  27.883333  29.383333  25.983333  \n",
      "1                         NaT  27.883333  29.383333  25.983333  \n",
      "2                         NaT  27.883333  29.383333  25.983333  \n",
      "3                         NaT  27.883333  29.383333  25.983333  \n",
      "4   2019-01-07 00:00:00+00:00  25.000000  26.500000  22.700000  \n",
      "..                        ...        ...        ...        ...  \n",
      "922 2024-11-11 00:00:00+00:00  28.400000  29.900000  26.800000  \n",
      "923 2024-11-25 00:00:00+00:00  28.600000  30.100000  27.200000  \n",
      "924 2024-11-25 00:00:00+00:00  28.600000  30.100000  27.200000  \n",
      "925 2024-11-25 00:00:00+00:00  28.600000  30.100000  27.200000  \n",
      "926 2024-11-25 00:00:00+00:00  28.600000  30.100000  27.200000  \n",
      "\n",
      "[927 rows x 16 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\$EKH000-V5FHVTC5DRPM\\AppData\\Local\\Temp\\ipykernel_7848\\3891609855.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[column_name] = pd.to_datetime(df[column_name], errors='coerce', utc=True)\n",
      "C:\\Users\\$EKH000-V5FHVTC5DRPM\\AppData\\Local\\Temp\\ipykernel_7848\\3891609855.py:68: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "''' \n",
    "Data Processing\n",
    "\n",
    "'''\n",
    "## \n",
    "\n",
    "# Merge regional data into a single DataFrame\n",
    "regional_data_1 = pd.concat([north_data, south_data, central_data, east_data], ignore_index=True)\n",
    "\n",
    "\n",
    "# Check and rename date columns if necessary\n",
    "def ensure_date_column(df, possible_names):\n",
    "    for col in df.columns:\n",
    "        if col in possible_names:\n",
    "            df.rename(columns={col: 'date'}, inplace=True)\n",
    "            break\n",
    "    return df\n",
    "\n",
    "# Rename the date columns where applicable\n",
    "regional_data = ensure_date_column(regional_data_1, ['週', 'date'])\n",
    "fuel_prices = ensure_date_column(fuel_prices, ['Date', 'date', '週', '日期'])\n",
    "cabbage_prices = ensure_date_column(cabbage_prices, ['週', 'date'])\n",
    "\n",
    "# Convert date columns to datetime\n",
    "def parse_date(df, column_name):\n",
    "    if column_name in df.columns:\n",
    "        df[column_name] = pd.to_datetime(df[column_name], errors='coerce', utc=True)\n",
    "    return df\n",
    "\n",
    "regional_data = parse_date(regional_data, 'date')\n",
    "fuel_prices = parse_date(fuel_prices, 'date')\n",
    "cabbage_prices = parse_date(cabbage_prices, 'date')\n",
    "\n",
    "# Drop rows with missing or invalid 'date' values\n",
    "for df in [regional_data, fuel_prices, cabbage_prices]:\n",
    "    if 'date' in df.columns:\n",
    "        df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Ensure 'date' columns are consistent and datetimelike\n",
    "for df in [regional_data, fuel_prices, cabbage_prices]:\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "\n",
    "# Extract additional features from date\n",
    "def extract_date_features(df, date_column):\n",
    "    df['year'] = df[date_column].dt.year\n",
    "    df['month'] = df[date_column].dt.month\n",
    "    df['week'] = df[date_column].dt.day // 7\n",
    "    return df\n",
    "\n",
    "cabbage_prices = extract_date_features(cabbage_prices, 'date')\n",
    "fuel_prices = extract_date_features(fuel_prices, 'date')\n",
    "\n",
    "# Merge all the data into a single DataFrame\n",
    "try:\n",
    "    merged_data = pd.merge(cabbage_prices, regional_data, on='date', how='left')\n",
    "    merged_data = pd.merge(merged_data, fuel_prices, on=['year', 'month', 'week'], how='left')\n",
    "except KeyError as e:\n",
    "    raise KeyError(f\"Error during merging: {e}. Please check that all dataframes contain a 'date' column.\")\n",
    "\n",
    "# 根據 Group 分組計算均值並填補缺失值\n",
    "merged_data['Fuel_92'] = merged_data.groupby('month')['Fuel_92'].transform(lambda x: x.fillna(x.mean()))\n",
    "merged_data['Fuel_95'] = merged_data.groupby('month')['Fuel_95'].transform(lambda x: x.fillna(x.mean()))\n",
    "merged_data['Fuel_High'] = merged_data.groupby('month')['Fuel_High'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Handle missing values\n",
    "merged_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Prepare features and target variable\n",
    "y = merged_data[['平均價', '交易量']]\n",
    "X = merged_data.drop(columns=['date_x', '平均價', '交易量', 'year', 'month', 'week', 'date_y'])\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "output_file = \"All_X.csv\"\n",
    "X.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Merged data saved to {output_file}\")\n",
    "\n",
    "print(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "''' \n",
    "Building the Model\n",
    "\n",
    "'''\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "''' \n",
    "Training the Model\n",
    "\n",
    "'''\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "''' \n",
    "Evaluation\n",
    "1. 哪種蔬果準確度最高，哪種最低，並分析原因\n",
    "\n",
    "'''\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to merged_data_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\$EKH000-V5FHVTC5DRPM\\AppData\\Local\\Temp\\ipykernel_7848\\2946272973.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[column_name] = pd.to_datetime(df[column_name], errors='coerce', utc=True)\n",
      "C:\\Users\\$EKH000-V5FHVTC5DRPM\\AppData\\Local\\Temp\\ipykernel_7848\\2946272973.py:55: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Merge regional data into a single DataFrame\n",
    "regional_data = pd.concat([north_data, south_data, central_data, east_data], ignore_index=True)\n",
    "\n",
    "# Check and rename date columns if necessary\n",
    "def ensure_date_column(df, possible_names):\n",
    "    for col in df.columns:\n",
    "        if col in possible_names:\n",
    "            df.rename(columns={col: 'date'}, inplace=True)\n",
    "            break\n",
    "    return df\n",
    "\n",
    "# Rename the date columns where applicable\n",
    "regional_data = ensure_date_column(regional_data, ['週', 'date'])\n",
    "fuel_prices = ensure_date_column(fuel_prices, ['Date', 'date', '週', '日期'])\n",
    "cabbage_prices = ensure_date_column(cabbage_prices, ['週', 'date'])\n",
    "\n",
    "# Convert date columns to datetime\n",
    "def parse_date(df, column_name):\n",
    "    if column_name in df.columns:\n",
    "        df[column_name] = pd.to_datetime(df[column_name], errors='coerce', utc=True)\n",
    "    return df\n",
    "\n",
    "regional_data = parse_date(regional_data, 'date')\n",
    "fuel_prices = parse_date(fuel_prices, 'date')\n",
    "cabbage_prices = parse_date(cabbage_prices, 'date')\n",
    "\n",
    "# Drop rows with missing or invalid 'date' values\n",
    "for df in [regional_data, fuel_prices, cabbage_prices]:\n",
    "    if 'date' in df.columns:\n",
    "        df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Ensure 'date' columns are consistent and datetimelike\n",
    "for df in [regional_data, fuel_prices, cabbage_prices]:\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "\n",
    "# Extract additional features from date\n",
    "def extract_date_features(df, date_column):\n",
    "    df['year'] = df[date_column].dt.year\n",
    "    df['month'] = df[date_column].dt.month\n",
    "    df['week'] = df[date_column].dt.isocalendar().week\n",
    "    df['day_of_week'] = df[date_column].dt.dayofweek\n",
    "    return df\n",
    "\n",
    "cabbage_prices = extract_date_features(cabbage_prices, 'date')\n",
    "\n",
    "# Merge all the data into a single DataFrame\n",
    "try:\n",
    "    merged_data = pd.merge(cabbage_prices, regional_data, on='date', how='left')\n",
    "    merged_data = pd.merge(merged_data, fuel_prices, on='date', how='left')\n",
    "except KeyError as e:\n",
    "    raise KeyError(f\"Error during merging: {e}. Please check that all dataframes contain a 'date' column.\")\n",
    "\n",
    "# Handle missing values\n",
    "merged_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Prepare features and target variable\n",
    "X = merged_data.drop(columns=['date', '平均價'])\n",
    "y = merged_data['平均價']\n",
    "\n",
    "# Impute missing values in features\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Set up the parameter grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'subsample': [0.8, 1.0],\n",
    "#     'colsample_bytree': [0.8, 1.0]\n",
    "# }\n",
    "\n",
    "# # Set up the XGBoost model\n",
    "# xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# # Set up GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "\n",
    "# # Fit the model using GridSearchCV\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best estimator and parameters\n",
    "# best_model = grid_search.best_estimator_\n",
    "# best_params = grid_search.best_params_\n",
    "# print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(f'Root Mean Squared Error: {rmse}')\n",
    "# print(f'Mean Absolute Error: {mae}')\n",
    "# print(f'R2 Score: {r2}')\n",
    "\n",
    "# # Save the best model\n",
    "# joblib.dump(best_model, 'cabbage_price_xgboost_best_model.pkl')\n",
    "\n",
    "# # Load and test the model\n",
    "# loaded_model = joblib.load('cabbage_price_xgboost_best_model.pkl')\n",
    "# loaded_y_pred = loaded_model.predict(X_test)\n",
    "# loaded_rmse = mean_squared_error(y_test, loaded_y_pred, squared=False)\n",
    "# print(f'Loaded Model Root Mean Squared Error: {loaded_rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# Load the data\n",
    "north_data = pd.read_csv('D:/DS_Prediction/Weather/north_weekly_averages.csv')\n",
    "south_data = pd.read_csv('D:/DS_Prediction/Weather/south_weekly_averages.csv')\n",
    "central_data = pd.read_csv('D:/DS_Prediction/Weather/central_weekly_averages.csv')\n",
    "east_data = pd.read_csv('D:/DS_Prediction/Weather/east_weekly_averages.csv')\n",
    "fuel_prices = pd.read_csv('D:/DS_Prediction/fuel_prices.csv')\n",
    "cabbage_prices = pd.read_csv('D:/DS_Prediction/國產包心菜.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\culture\\AppData\\Local\\Temp\\ipykernel_23884\\2805110794.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[column_name] = pd.to_datetime(df[column_name], errors='coerce', utc=True)\n",
      "C:\\Users\\culture\\AppData\\Local\\Temp\\ipykernel_23884\\2805110794.py:55: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Root Mean Squared Error: 5.984999371397111\n",
      "Mean Absolute Error: 3.033320808642719\n",
      "R2 Score: 0.8938061673671138\n",
      "Loaded Model Root Mean Squared Error: 5.984999371397111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\culture\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\culture\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Merge regional data into a single DataFrame\n",
    "regional_data = pd.concat([north_data, south_data, central_data, east_data], ignore_index=True)\n",
    "\n",
    "# Check and rename date columns if necessary\n",
    "def ensure_date_column(df, possible_names):\n",
    "    for col in df.columns:\n",
    "        if col in possible_names:\n",
    "            df.rename(columns={col: 'date'}, inplace=True)\n",
    "            break\n",
    "    return df\n",
    "\n",
    "# Rename the date columns where applicable\n",
    "regional_data = ensure_date_column(regional_data, ['週', 'date'])\n",
    "fuel_prices = ensure_date_column(fuel_prices, ['Date', 'date', '週', '日期'])\n",
    "cabbage_prices = ensure_date_column(cabbage_prices, ['週', 'date'])\n",
    "\n",
    "# Convert date columns to datetime\n",
    "def parse_date(df, column_name):\n",
    "    if column_name in df.columns:\n",
    "        df[column_name] = pd.to_datetime(df[column_name], errors='coerce', utc=True)\n",
    "    return df\n",
    "\n",
    "regional_data = parse_date(regional_data, 'date')\n",
    "fuel_prices = parse_date(fuel_prices, 'date')\n",
    "cabbage_prices = parse_date(cabbage_prices, 'date')\n",
    "\n",
    "# Drop rows with missing or invalid 'date' values\n",
    "for df in [regional_data, fuel_prices, cabbage_prices]:\n",
    "    if 'date' in df.columns:\n",
    "        df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# Ensure 'date' columns are consistent and datetimelike\n",
    "for df in [regional_data, fuel_prices, cabbage_prices]:\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "\n",
    "# Extract additional features from date\n",
    "def extract_date_features(df, date_column):\n",
    "    df['year'] = df[date_column].dt.year\n",
    "    df['month'] = df[date_column].dt.month\n",
    "    df['week'] = df[date_column].dt.isocalendar().week\n",
    "    df['day_of_week'] = df[date_column].dt.dayofweek\n",
    "    return df\n",
    "\n",
    "cabbage_prices = extract_date_features(cabbage_prices, 'date')\n",
    "\n",
    "# Merge all the data into a single DataFrame\n",
    "try:\n",
    "    merged_data = pd.merge(cabbage_prices, regional_data, on='date', how='left')\n",
    "    merged_data = pd.merge(merged_data, fuel_prices, on='date', how='left')\n",
    "except KeyError as e:\n",
    "    raise KeyError(f\"Error during merging: {e}. Please check that all dataframes contain a 'date' column.\")\n",
    "\n",
    "# Handle missing values\n",
    "merged_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Prepare features and target variable\n",
    "X = merged_data.drop(columns=['date', '平均價'])\n",
    "y = merged_data['平均價']\n",
    "\n",
    "# Impute missing values in features\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set up the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Set up the SVR model\n",
    "svr_model = SVR()\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svr_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "\n",
    "# Fit the model using GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator and parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Parameters: {best_params}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'R2 Score: {r2}')\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'cabbage_price_svr_best_model.pkl')\n",
    "\n",
    "# Load and test the model\n",
    "loaded_model = joblib.load('cabbage_price_svr_best_model.pkl')\n",
    "loaded_y_pred = loaded_model.predict(X_test)\n",
    "loaded_rmse = mean_squared_error(y_test, loaded_y_pred, squared=False)\n",
    "print(f'Loaded Model Root Mean Squared Error: {loaded_rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN+Transfermor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "# Load the data\n",
    "north_data = pd.read_csv('D:/DS_Prediction/Weather/north_weekly_averages.csv')\n",
    "south_data = pd.read_csv('D:/DS_Prediction/Weather/south_weekly_averages.csv')\n",
    "central_data = pd.read_csv('D:/DS_Prediction/Weather/central_weekly_averages.csv')\n",
    "east_data = pd.read_csv('D:/DS_Prediction/Weather/east_weekly_averages.csv')\n",
    "fuel_prices = pd.read_csv('D:/DS_Prediction/fuel_prices.csv')\n",
    "cabbage_prices = pd.read_csv('D:/DS_Prediction/國產包心菜.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\culture\\AppData\\Local\\Temp\\ipykernel_23884\\3479751801.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[column_name] = pd.to_datetime(df[column_name], errors='coerce', utc=True)\n",
      "C:\\Users\\culture\\AppData\\Local\\Temp\\ipykernel_23884\\3479751801.py:55: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_17 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_62 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_51 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_63 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_52 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">41,473</span> (162.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m41,473\u001b[0m (162.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">41,473</span> (162.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m41,473\u001b[0m (162.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 1826.0809 - val_loss: 809.1030\n",
      "Epoch 2/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 703.3657 - val_loss: 455.3844\n",
      "Epoch 3/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 403.6914 - val_loss: 423.7955\n",
      "Epoch 4/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 388.7868 - val_loss: 391.0662\n",
      "Epoch 5/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 370.1884 - val_loss: 380.6324\n",
      "Epoch 6/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 363.6079 - val_loss: 354.7948\n",
      "Epoch 7/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 326.3534 - val_loss: 331.4171\n",
      "Epoch 8/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 324.1828 - val_loss: 339.9662\n",
      "Epoch 9/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 299.7538 - val_loss: 304.8896\n",
      "Epoch 10/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 286.0587 - val_loss: 282.1664\n",
      "Epoch 11/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 269.1656 - val_loss: 282.6281\n",
      "Epoch 12/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 252.9012 - val_loss: 259.7356\n",
      "Epoch 13/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 253.9319 - val_loss: 253.3118\n",
      "Epoch 14/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 219.2617 - val_loss: 237.1134\n",
      "Epoch 15/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 242.4212 - val_loss: 231.7016\n",
      "Epoch 16/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 215.6790 - val_loss: 223.8292\n",
      "Epoch 17/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 204.3357 - val_loss: 203.0875\n",
      "Epoch 18/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 197.5084 - val_loss: 191.8464\n",
      "Epoch 19/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 178.6996 - val_loss: 188.7191\n",
      "Epoch 20/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 162.6815 - val_loss: 166.0254\n",
      "Epoch 21/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 151.1673 - val_loss: 156.0042\n",
      "Epoch 22/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 153.0914 - val_loss: 143.1182\n",
      "Epoch 23/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 137.1273 - val_loss: 128.5772\n",
      "Epoch 24/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 107.2279 - val_loss: 116.5070\n",
      "Epoch 25/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 114.0337 - val_loss: 111.9143\n",
      "Epoch 26/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 106.9700 - val_loss: 108.0000\n",
      "Epoch 27/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 106.2520 - val_loss: 98.5175\n",
      "Epoch 28/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 103.6716 - val_loss: 92.3450\n",
      "Epoch 29/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 97.3403 - val_loss: 95.1566\n",
      "Epoch 30/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 95.5680 - val_loss: 90.9362\n",
      "Epoch 31/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 92.2712 - val_loss: 84.3591\n",
      "Epoch 32/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 83.8553 - val_loss: 90.8919\n",
      "Epoch 33/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 96.2006 - val_loss: 82.5595\n",
      "Epoch 34/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 80.6028 - val_loss: 82.1264\n",
      "Epoch 35/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 86.8470 - val_loss: 83.8071\n",
      "Epoch 36/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 83.6225 - val_loss: 79.2236\n",
      "Epoch 37/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 76.7391 - val_loss: 80.0957\n",
      "Epoch 38/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 82.8170 - val_loss: 84.0707\n",
      "Epoch 39/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 82.6990 - val_loss: 94.7046\n",
      "Epoch 40/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 91.6882 - val_loss: 103.6402\n",
      "Epoch 41/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 89.2645 - val_loss: 79.5726\n",
      "Epoch 42/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 81.4916 - val_loss: 78.9950\n",
      "Epoch 43/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 78.2625 - val_loss: 75.7868\n",
      "Epoch 44/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 80.3456 - val_loss: 75.4000\n",
      "Epoch 45/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 73.3757 - val_loss: 78.6887\n",
      "Epoch 46/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 82.0310 - val_loss: 74.4238\n",
      "Epoch 47/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 75.6887 - val_loss: 73.1451\n",
      "Epoch 48/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 66.1918 - val_loss: 74.9464\n",
      "Epoch 49/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 65.1834 - val_loss: 73.3324\n",
      "Epoch 50/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 61.7110 - val_loss: 74.7059\n",
      "Epoch 51/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 59.8916 - val_loss: 72.1477\n",
      "Epoch 52/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 73.7025 - val_loss: 82.1585\n",
      "Epoch 53/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 74.0207 - val_loss: 70.4660\n",
      "Epoch 54/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 62.1815 - val_loss: 73.6721\n",
      "Epoch 55/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 67.0221 - val_loss: 69.8897\n",
      "Epoch 56/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 62.5429 - val_loss: 72.8562\n",
      "Epoch 57/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 57.9070 - val_loss: 68.7941\n",
      "Epoch 58/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 60.2514 - val_loss: 75.2365\n",
      "Epoch 59/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 60.3837 - val_loss: 71.5134\n",
      "Epoch 60/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 63.5066 - val_loss: 70.2584\n",
      "Epoch 61/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 62.5875 - val_loss: 73.1170\n",
      "Epoch 62/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 66.2507 - val_loss: 71.2120\n",
      "Epoch 63/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 60.9077 - val_loss: 69.0974\n",
      "Epoch 64/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 58.7202 - val_loss: 74.4875\n",
      "Epoch 65/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 58.6113 - val_loss: 70.4045\n",
      "Epoch 66/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 65.5183 - val_loss: 70.6156\n",
      "Epoch 67/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 56.7462 - val_loss: 70.2522\n",
      "Epoch 68/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 55.8602 - val_loss: 74.7862\n",
      "Epoch 69/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 64.4759 - val_loss: 68.3214\n",
      "Epoch 70/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 56.9918 - val_loss: 71.4105\n",
      "Epoch 71/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 54.2339 - val_loss: 68.1644\n",
      "Epoch 72/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 58.2272 - val_loss: 68.4757\n",
      "Epoch 73/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 58.7623 - val_loss: 72.2116\n",
      "Epoch 74/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 62.5351 - val_loss: 69.0298\n",
      "Epoch 75/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 54.0443 - val_loss: 69.3932\n",
      "Epoch 76/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 57.5581 - val_loss: 66.3985\n",
      "Epoch 77/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 51.6881 - val_loss: 70.5547\n",
      "Epoch 78/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 57.8899 - val_loss: 67.4828\n",
      "Epoch 79/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 50.7793 - val_loss: 67.8601\n",
      "Epoch 80/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 46.0084 - val_loss: 69.0769\n",
      "Epoch 81/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 48.8068 - val_loss: 70.2713\n",
      "Epoch 82/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 52.0286 - val_loss: 71.2073\n",
      "Epoch 83/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 51.3463 - val_loss: 71.0504\n",
      "Epoch 84/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 50.7512 - val_loss: 65.3784\n",
      "Epoch 85/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 53.2983 - val_loss: 75.6296\n",
      "Epoch 86/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 52.2118 - val_loss: 74.9351\n",
      "Epoch 87/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 53.0130 - val_loss: 68.6994\n",
      "Epoch 88/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 43.4558 - val_loss: 67.0519\n",
      "Epoch 89/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 46.7108 - val_loss: 69.2977\n",
      "Epoch 90/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 47.3957 - val_loss: 65.6128\n",
      "Epoch 91/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 47.9238 - val_loss: 68.3825\n",
      "Epoch 92/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 46.6941 - val_loss: 67.9654\n",
      "Epoch 93/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 43.8643 - val_loss: 66.6677\n",
      "Epoch 94/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 42.1968 - val_loss: 68.4536\n",
      "Epoch 95/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 45.9856 - val_loss: 76.6454\n",
      "Epoch 96/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 48.4734 - val_loss: 67.2698\n",
      "Epoch 97/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 43.2525 - val_loss: 69.5243\n",
      "Epoch 98/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 39.7494 - val_loss: 72.5706\n",
      "Epoch 99/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 45.9896 - val_loss: 65.0265\n",
      "Epoch 100/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 40.8207 - val_loss: 64.7169\n",
      "Epoch 101/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 38.3118 - val_loss: 66.8126\n",
      "Epoch 102/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 42.4713 - val_loss: 67.6821\n",
      "Epoch 103/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 38.4993 - val_loss: 72.6157\n",
      "Epoch 104/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 48.4113 - val_loss: 70.6225\n",
      "Epoch 105/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 43.8446 - val_loss: 69.2409\n",
      "Epoch 106/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 38.9371 - val_loss: 75.0685\n",
      "Epoch 107/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 39.3017 - val_loss: 67.6603\n",
      "Epoch 108/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 35.0866 - val_loss: 66.4331\n",
      "Epoch 109/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 36.1359 - val_loss: 65.3489\n",
      "Epoch 110/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 37.4395 - val_loss: 65.6324\n",
      "Epoch 111/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 39.9050 - val_loss: 64.6240\n",
      "Epoch 112/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 36.5793 - val_loss: 77.5923\n",
      "Epoch 113/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 40.7818 - val_loss: 65.5766\n",
      "Epoch 114/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 31.5378 - val_loss: 68.5365\n",
      "Epoch 115/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 43.7633 - val_loss: 73.6778\n",
      "Epoch 116/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 34.8502 - val_loss: 74.6354\n",
      "Epoch 117/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 34.6633 - val_loss: 65.2433\n",
      "Epoch 118/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 33.9909 - val_loss: 64.3601\n",
      "Epoch 119/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 34.1280 - val_loss: 62.8036\n",
      "Epoch 120/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 36.5140 - val_loss: 70.8315\n",
      "Epoch 121/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 38.9254 - val_loss: 74.7402\n",
      "Epoch 122/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 35.8651 - val_loss: 64.8362\n",
      "Epoch 123/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 32.7101 - val_loss: 73.0929\n",
      "Epoch 124/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 48.5703 - val_loss: 62.6352\n",
      "Epoch 125/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 31.7194 - val_loss: 66.5720\n",
      "Epoch 126/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 31.5564 - val_loss: 63.2932\n",
      "Epoch 127/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 29.7855 - val_loss: 63.7639\n",
      "Epoch 128/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 35.2047 - val_loss: 71.6521\n",
      "Epoch 129/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 36.3765 - val_loss: 63.3799\n",
      "Epoch 130/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 28.3416 - val_loss: 63.9421\n",
      "Epoch 131/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 27.8781 - val_loss: 62.7465\n",
      "Epoch 132/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 31.3447 - val_loss: 64.4323\n",
      "Epoch 133/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 29.4472 - val_loss: 61.9558\n",
      "Epoch 134/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25.7178 - val_loss: 64.8140\n",
      "Epoch 135/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 27.2195 - val_loss: 64.8997\n",
      "Epoch 136/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 35.0255 - val_loss: 82.4956\n",
      "Epoch 137/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 32.3193 - val_loss: 63.8342\n",
      "Epoch 138/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25.2070 - val_loss: 64.0249\n",
      "Epoch 139/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24.2298 - val_loss: 63.9865\n",
      "Epoch 140/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 25.4123 - val_loss: 66.3635\n",
      "Epoch 141/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 24.2500 - val_loss: 62.7590\n",
      "Epoch 142/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 30.3857 - val_loss: 68.1858\n",
      "Epoch 143/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 28.3306 - val_loss: 61.9214\n",
      "Epoch 144/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 27.9177 - val_loss: 62.9117\n",
      "Epoch 145/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.3048 - val_loss: 67.9624\n",
      "Epoch 146/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.3993 - val_loss: 68.3689\n",
      "Epoch 147/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 27.2719 - val_loss: 63.4324\n",
      "Epoch 148/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 23.3077 - val_loss: 65.1190\n",
      "Epoch 149/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 26.9262 - val_loss: 63.6667\n",
      "Epoch 150/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 25.2635 - val_loss: 63.9536\n",
      "Epoch 151/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.6284 - val_loss: 61.3020\n",
      "Epoch 152/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.8338 - val_loss: 66.2305\n",
      "Epoch 153/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.7587 - val_loss: 63.9301\n",
      "Epoch 154/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 29.6655 - val_loss: 65.7290\n",
      "Epoch 155/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 25.6870 - val_loss: 65.6879\n",
      "Epoch 156/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.5094 - val_loss: 63.9470\n",
      "Epoch 157/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.1189 - val_loss: 61.8911\n",
      "Epoch 158/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.4300 - val_loss: 63.7378\n",
      "Epoch 159/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.6259 - val_loss: 62.6244\n",
      "Epoch 160/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.4286 - val_loss: 63.0710\n",
      "Epoch 161/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 22.1312 - val_loss: 64.7466\n",
      "Epoch 162/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.0110 - val_loss: 61.2345\n",
      "Epoch 163/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17.5312 - val_loss: 67.8911\n",
      "Epoch 164/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.5517 - val_loss: 65.9813\n",
      "Epoch 165/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25.2013 - val_loss: 62.5345\n",
      "Epoch 166/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 20.4680 - val_loss: 65.6765\n",
      "Epoch 167/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 27.9706 - val_loss: 61.7289\n",
      "Epoch 168/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18.9477 - val_loss: 63.1784\n",
      "Epoch 169/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 18.1803 - val_loss: 61.8129\n",
      "Epoch 170/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.0992 - val_loss: 63.3813\n",
      "Epoch 171/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 20.5248 - val_loss: 61.5610\n",
      "Epoch 172/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.2180 - val_loss: 71.2681\n",
      "Epoch 173/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.4764 - val_loss: 65.4976\n",
      "Epoch 174/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.2205 - val_loss: 67.3736\n",
      "Epoch 175/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.2784 - val_loss: 72.4992\n",
      "Epoch 176/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 22.2827 - val_loss: 62.8145\n",
      "Epoch 177/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.1884 - val_loss: 69.4045\n",
      "Epoch 178/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.3110 - val_loss: 63.4818\n",
      "Epoch 179/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.7437 - val_loss: 61.4267\n",
      "Epoch 180/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.5720 - val_loss: 62.4412\n",
      "Epoch 181/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.3214 - val_loss: 61.3044\n",
      "Epoch 182/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.8547 - val_loss: 63.0590\n",
      "Epoch 183/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.5996 - val_loss: 63.3778\n",
      "Epoch 184/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.8349 - val_loss: 61.4456\n",
      "Epoch 185/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 16.0331 - val_loss: 61.1215\n",
      "Epoch 186/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.3105 - val_loss: 62.8017\n",
      "Epoch 187/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.8233 - val_loss: 59.3635\n",
      "Epoch 188/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 14.3005 - val_loss: 61.4745\n",
      "Epoch 189/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.0773 - val_loss: 64.9089\n",
      "Epoch 190/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.4213 - val_loss: 65.5743\n",
      "Epoch 191/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 15.0289 - val_loss: 77.4863\n",
      "Epoch 192/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 21.9871 - val_loss: 63.5947\n",
      "Epoch 193/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.9794 - val_loss: 60.5413\n",
      "Epoch 194/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 15.3420 - val_loss: 62.4285\n",
      "Epoch 195/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.9933 - val_loss: 72.5730\n",
      "Epoch 196/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24.6094 - val_loss: 62.4385\n",
      "Epoch 197/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.0774 - val_loss: 61.6410\n",
      "Epoch 198/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 12.9556 - val_loss: 64.8703\n",
      "Epoch 199/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 13.2956 - val_loss: 67.1267\n",
      "Epoch 200/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 13.2148 - val_loss: 66.1702\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\culture\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 8.134504816459765\n",
      "Mean Absolute Error: 5.8534705413285115\n",
      "R2 Score: 0.8038296720186959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Loaded Model Root Mean Squared Error: 8.134504816459765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\culture\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 合并区域数据\n",
    "regional_data = pd.concat([north_data, south_data, central_data, east_data], ignore_index=True)\n",
    "\n",
    "# 处理日期列\n",
    "def ensure_date_column(df, possible_names):\n",
    "    for col in df.columns:\n",
    "        if col in possible_names:\n",
    "            df.rename(columns={col: 'date'}, inplace=True)\n",
    "            break\n",
    "    return df\n",
    "\n",
    "# 重命名日期列\n",
    "regional_data = ensure_date_column(regional_data, ['週', 'date'])\n",
    "fuel_prices = ensure_date_column(fuel_prices, ['Date', 'date', '週', '日期'])\n",
    "cabbage_prices = ensure_date_column(cabbage_prices, ['週', 'date'])\n",
    "\n",
    "# 转换日期列\n",
    "def parse_date(df, column_name):\n",
    "    if column_name in df.columns:\n",
    "        df[column_name] = pd.to_datetime(df[column_name], errors='coerce', utc=True)\n",
    "    return df\n",
    "\n",
    "regional_data = parse_date(regional_data, 'date')\n",
    "fuel_prices = parse_date(fuel_prices, 'date')\n",
    "cabbage_prices = parse_date(cabbage_prices, 'date')\n",
    "\n",
    "# 去除缺失的日期值\n",
    "for df in [regional_data, fuel_prices, cabbage_prices]:\n",
    "    if 'date' in df.columns:\n",
    "        df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "# 确保日期列一致并标准化\n",
    "for df in [regional_data, fuel_prices, cabbage_prices]:\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "\n",
    "# 提取日期特征\n",
    "def extract_date_features(df, date_column):\n",
    "    df['year'] = df[date_column].dt.year\n",
    "    df['month'] = df[date_column].dt.month\n",
    "    df['week'] = df[date_column].dt.isocalendar().week\n",
    "    df['day_of_week'] = df[date_column].dt.dayofweek\n",
    "    return df\n",
    "\n",
    "cabbage_prices = extract_date_features(cabbage_prices, 'date')\n",
    "\n",
    "# 合并数据\n",
    "try:\n",
    "    merged_data = pd.merge(cabbage_prices, regional_data, on='date', how='left')\n",
    "    merged_data = pd.merge(merged_data, fuel_prices, on='date', how='left')\n",
    "except KeyError as e:\n",
    "    raise KeyError(f\"合并数据时发生错误: {e}. 请检查所有数据框中是否包含 'date' 列.\")\n",
    "\n",
    "# 处理缺失值\n",
    "merged_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# 准备特征和目标变量\n",
    "X = merged_data.drop(columns=['date', '平均價'])\n",
    "y = merged_data['平均價']\n",
    "\n",
    "# 确保 X 和 y 的样本数量一致\n",
    "assert X.shape[0] == y.shape[0], \"X 和 y 的样本数量不一致!\"\n",
    "\n",
    "# 处理缺失的特征值\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# 标准化特征\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 重塑输入形状以适应CNN\n",
    "X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "def build_simplified_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # CNN 层\n",
    "    x = layers.Conv1D(64, 3, activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(2)(x)  # 池化操作\n",
    "    x = layers.Conv1D(128, 3, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)  # 池化操作\n",
    "    \n",
    "    # Flatten 层：将卷积输出展平成一维\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    # 全连接层\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dense(1)(x)  # 输出预测值\n",
    "\n",
    "    # 创建模型\n",
    "    model = models.Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# 创建模型并编译\n",
    "model = build_simplified_model((X_train.shape[1], 1))\n",
    "optimizer = optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# 打印模型概述\n",
    "model.summary()\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 评估模型\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "\n",
    "# 保存模型\n",
    "model.save('cabbage_price_cnn_transformer_model.h5')\n",
    "\n",
    "# 加载并测试模型\n",
    "loaded_model = models.load_model('cabbage_price_cnn_transformer_model.h5')\n",
    "loaded_y_pred = loaded_model.predict(X_test)\n",
    "loaded_rmse = mean_squared_error(y_test, loaded_y_pred, squared=False)\n",
    "print(f\"Loaded Model Root Mean Squared Error: {loaded_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_19 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m394,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">551,169</span> (2.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m551,169\u001b[0m (2.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">551,169</span> (2.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m551,169\u001b[0m (2.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 55ms/step - loss: 1424.7906 - val_loss: 381.2544\n",
      "Epoch 2/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 364.8336 - val_loss: 385.7947\n",
      "Epoch 3/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 366.1913 - val_loss: 342.1292\n",
      "Epoch 4/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 383.2879 - val_loss: 340.4164\n",
      "Epoch 5/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 348.7037 - val_loss: 342.9824\n",
      "Epoch 6/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 338.6981 - val_loss: 362.4497\n",
      "Epoch 7/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 380.7781 - val_loss: 337.0110\n",
      "Epoch 8/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 365.9332 - val_loss: 335.4888\n",
      "Epoch 9/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 341.1794 - val_loss: 338.2474\n",
      "Epoch 10/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 332.8483 - val_loss: 346.8173\n",
      "Epoch 11/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 345.3005 - val_loss: 364.0477\n",
      "Epoch 12/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 390.0128 - val_loss: 390.4794\n",
      "Epoch 13/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 357.5881 - val_loss: 383.7589\n",
      "Epoch 14/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 383.7438 - val_loss: 355.8273\n",
      "Epoch 15/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 361.9527 - val_loss: 348.6518\n",
      "Epoch 16/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 368.3224 - val_loss: 357.0332\n",
      "Epoch 17/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 339.3700 - val_loss: 335.2398\n",
      "Epoch 18/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 356.0502 - val_loss: 340.6482\n",
      "Epoch 19/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 336.7447 - val_loss: 365.7631\n",
      "Epoch 20/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 354.5337 - val_loss: 337.0175\n",
      "Epoch 21/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 331.4657 - val_loss: 359.6399\n",
      "Epoch 22/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 318.6233 - val_loss: 323.2611\n",
      "Epoch 23/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 322.6422 - val_loss: 378.9185\n",
      "Epoch 24/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 312.1352 - val_loss: 338.5935\n",
      "Epoch 25/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 331.5197 - val_loss: 326.9326\n",
      "Epoch 26/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 299.4548 - val_loss: 313.1952\n",
      "Epoch 27/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 303.0580 - val_loss: 322.2931\n",
      "Epoch 28/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 317.4965 - val_loss: 306.6508\n",
      "Epoch 29/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 311.6398 - val_loss: 304.7940\n",
      "Epoch 30/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 305.6398 - val_loss: 304.3478\n",
      "Epoch 31/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 269.7854 - val_loss: 304.7289\n",
      "Epoch 32/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 289.0703 - val_loss: 290.8495\n",
      "Epoch 33/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 284.4792 - val_loss: 290.5634\n",
      "Epoch 34/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 253.9875 - val_loss: 278.0524\n",
      "Epoch 35/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 272.9480 - val_loss: 275.4679\n",
      "Epoch 36/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 262.2227 - val_loss: 305.9482\n",
      "Epoch 37/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 278.4050 - val_loss: 288.0272\n",
      "Epoch 38/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 260.7230 - val_loss: 298.8802\n",
      "Epoch 39/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 271.7928 - val_loss: 319.6212\n",
      "Epoch 40/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 271.3210 - val_loss: 284.6456\n",
      "Epoch 41/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 239.7869 - val_loss: 276.1042\n",
      "Epoch 42/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 271.2271 - val_loss: 254.8593\n",
      "Epoch 43/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 287.1845 - val_loss: 266.8654\n",
      "Epoch 44/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 245.7460 - val_loss: 261.2526\n",
      "Epoch 45/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 241.3972 - val_loss: 240.1263\n",
      "Epoch 46/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 245.4877 - val_loss: 235.0098\n",
      "Epoch 47/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 239.4790 - val_loss: 244.5530\n",
      "Epoch 48/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 248.8685 - val_loss: 239.7089\n",
      "Epoch 49/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 238.2996 - val_loss: 234.2763\n",
      "Epoch 50/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 199.2496 - val_loss: 214.5665\n",
      "Epoch 51/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 223.9596 - val_loss: 197.8951\n",
      "Epoch 52/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 212.9592 - val_loss: 212.2554\n",
      "Epoch 53/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 205.2436 - val_loss: 243.8661\n",
      "Epoch 54/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 241.9022 - val_loss: 245.2813\n",
      "Epoch 55/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 230.1018 - val_loss: 229.0045\n",
      "Epoch 56/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 207.2249 - val_loss: 213.0115\n",
      "Epoch 57/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 198.7907 - val_loss: 192.7249\n",
      "Epoch 58/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 163.9157 - val_loss: 179.9951\n",
      "Epoch 59/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 169.0117 - val_loss: 174.6906\n",
      "Epoch 60/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 165.8366 - val_loss: 190.7921\n",
      "Epoch 61/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 170.4431 - val_loss: 159.1529\n",
      "Epoch 62/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 146.0038 - val_loss: 150.1732\n",
      "Epoch 63/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 131.3111 - val_loss: 142.9959\n",
      "Epoch 64/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 145.5390 - val_loss: 158.5115\n",
      "Epoch 65/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 131.0576 - val_loss: 132.6661\n",
      "Epoch 66/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 126.7571 - val_loss: 151.0760\n",
      "Epoch 67/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 147.3671 - val_loss: 120.4019\n",
      "Epoch 68/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 124.2402 - val_loss: 126.8991\n",
      "Epoch 69/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 136.1835 - val_loss: 114.9061\n",
      "Epoch 70/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 118.3686 - val_loss: 107.2668\n",
      "Epoch 71/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 123.9884 - val_loss: 123.1968\n",
      "Epoch 72/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 145.4065 - val_loss: 123.5445\n",
      "Epoch 73/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 121.8452 - val_loss: 136.7592\n",
      "Epoch 74/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 123.2094 - val_loss: 119.3002\n",
      "Epoch 75/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 121.5979 - val_loss: 184.3760\n",
      "Epoch 76/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 135.2328 - val_loss: 121.9966\n",
      "Epoch 77/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 103.6964 - val_loss: 99.5422\n",
      "Epoch 78/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 110.3550 - val_loss: 94.3355\n",
      "Epoch 79/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 109.7358 - val_loss: 93.8566\n",
      "Epoch 80/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 98.0792 - val_loss: 87.1031\n",
      "Epoch 81/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 96.4445 - val_loss: 88.3632\n",
      "Epoch 82/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 81.1522 - val_loss: 103.9344\n",
      "Epoch 83/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 97.5003 - val_loss: 86.9173\n",
      "Epoch 84/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 77.0951 - val_loss: 109.4620\n",
      "Epoch 85/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 130.4646 - val_loss: 84.1029\n",
      "Epoch 86/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 82.1223 - val_loss: 77.0509\n",
      "Epoch 87/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 76.7726 - val_loss: 80.1383\n",
      "Epoch 88/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 81.4387 - val_loss: 86.8352\n",
      "Epoch 89/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 77.4891 - val_loss: 79.8764\n",
      "Epoch 90/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 69.8473 - val_loss: 120.5232\n",
      "Epoch 91/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 101.2000 - val_loss: 111.4779\n",
      "Epoch 92/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 116.1970 - val_loss: 98.0361\n",
      "Epoch 93/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 111.4883 - val_loss: 101.1556\n",
      "Epoch 94/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 99.0436 - val_loss: 88.2224\n",
      "Epoch 95/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 99.3567 - val_loss: 99.4662\n",
      "Epoch 96/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 100.2399 - val_loss: 91.2478\n",
      "Epoch 97/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 88.0498 - val_loss: 83.2506\n",
      "Epoch 98/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 87.4964 - val_loss: 89.7330\n",
      "Epoch 99/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 78.0446 - val_loss: 83.6897\n",
      "Epoch 100/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 81.2349 - val_loss: 77.5899\n",
      "Epoch 101/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 82.6520 - val_loss: 77.4219\n",
      "Epoch 102/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 76.7659 - val_loss: 74.3927\n",
      "Epoch 103/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 85.6306 - val_loss: 77.4092\n",
      "Epoch 104/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 76.5842 - val_loss: 79.9169\n",
      "Epoch 105/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 90.1169 - val_loss: 72.2484\n",
      "Epoch 106/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 85.6522 - val_loss: 75.5506\n",
      "Epoch 107/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 77.8678 - val_loss: 74.2101\n",
      "Epoch 108/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 85.0610 - val_loss: 82.6173\n",
      "Epoch 109/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 88.8096 - val_loss: 72.5108\n",
      "Epoch 110/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 75.4790 - val_loss: 70.4516\n",
      "Epoch 111/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 68.0584 - val_loss: 69.6823\n",
      "Epoch 112/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 75.6556 - val_loss: 75.9108\n",
      "Epoch 113/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 67.8045 - val_loss: 63.1448\n",
      "Epoch 114/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 70.7564 - val_loss: 81.5229\n",
      "Epoch 115/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 71.5343 - val_loss: 85.4783\n",
      "Epoch 116/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 68.8394 - val_loss: 69.7387\n",
      "Epoch 117/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 73.8142 - val_loss: 69.3436\n",
      "Epoch 118/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 67.8441 - val_loss: 64.8233\n",
      "Epoch 119/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 62.5391 - val_loss: 59.6572\n",
      "Epoch 120/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 63.6687 - val_loss: 58.8654\n",
      "Epoch 121/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 67.9589 - val_loss: 57.8251\n",
      "Epoch 122/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 60.1967 - val_loss: 61.7217\n",
      "Epoch 123/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 60.7036 - val_loss: 55.6666\n",
      "Epoch 124/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 58.3507 - val_loss: 61.0348\n",
      "Epoch 125/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 64.4846 - val_loss: 55.9327\n",
      "Epoch 126/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 59.5517 - val_loss: 57.1463\n",
      "Epoch 127/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 53.7548 - val_loss: 57.3156\n",
      "Epoch 128/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 57.7999 - val_loss: 73.3332\n",
      "Epoch 129/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 58.5650 - val_loss: 57.3265\n",
      "Epoch 130/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 57.5671 - val_loss: 58.2286\n",
      "Epoch 131/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 67.8328 - val_loss: 69.1272\n",
      "Epoch 132/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 54.8168 - val_loss: 53.0185\n",
      "Epoch 133/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 57.3356 - val_loss: 47.3633\n",
      "Epoch 134/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 51.0009 - val_loss: 55.9483\n",
      "Epoch 135/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 46.5569 - val_loss: 53.1029\n",
      "Epoch 136/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 52.7195 - val_loss: 57.1257\n",
      "Epoch 137/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 52.9752 - val_loss: 50.6596\n",
      "Epoch 138/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 46.2138 - val_loss: 59.5781\n",
      "Epoch 139/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 46.3586 - val_loss: 43.3956\n",
      "Epoch 140/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 49.2672 - val_loss: 52.2320\n",
      "Epoch 141/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 46.3145 - val_loss: 45.5534\n",
      "Epoch 142/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 54.8088 - val_loss: 60.4150\n",
      "Epoch 143/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 52.0007 - val_loss: 48.4156\n",
      "Epoch 144/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 51.3843 - val_loss: 52.3473\n",
      "Epoch 145/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 55.1850 - val_loss: 101.4233\n",
      "Epoch 146/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 73.5642 - val_loss: 52.8744\n",
      "Epoch 147/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 50.8674 - val_loss: 44.9076\n",
      "Epoch 148/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 45.9343 - val_loss: 47.1937\n",
      "Epoch 149/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 38.3066 - val_loss: 57.6895\n",
      "Epoch 150/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 42.7211 - val_loss: 47.1018\n",
      "Epoch 151/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 35.4099 - val_loss: 41.0665\n",
      "Epoch 152/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 41.9451 - val_loss: 63.5981\n",
      "Epoch 153/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 40.9295 - val_loss: 43.9822\n",
      "Epoch 154/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 41.6181 - val_loss: 45.2955\n",
      "Epoch 155/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 41.4167 - val_loss: 48.5908\n",
      "Epoch 156/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 41.8196 - val_loss: 47.7800\n",
      "Epoch 157/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 36.8921 - val_loss: 48.2519\n",
      "Epoch 158/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 41.4212 - val_loss: 40.6037\n",
      "Epoch 159/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 36.4744 - val_loss: 47.9401\n",
      "Epoch 160/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 56.5759 - val_loss: 52.2002\n",
      "Epoch 161/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 45.9166 - val_loss: 50.2157\n",
      "Epoch 162/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 44.1338 - val_loss: 42.1109\n",
      "Epoch 163/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 33.9544 - val_loss: 47.7041\n",
      "Epoch 164/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 35.5036 - val_loss: 46.3238\n",
      "Epoch 165/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 36.5202 - val_loss: 51.9592\n",
      "Epoch 166/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 38.6828 - val_loss: 46.1070\n",
      "Epoch 167/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 36.3504 - val_loss: 42.1461\n",
      "Epoch 168/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 43.0513 - val_loss: 66.0603\n",
      "Epoch 169/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 43.1697 - val_loss: 42.2433\n",
      "Epoch 170/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 37.9544 - val_loss: 43.6031\n",
      "Epoch 171/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 34.1851 - val_loss: 44.0235\n",
      "Epoch 172/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 40.1231 - val_loss: 52.9606\n",
      "Epoch 173/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 33.2754 - val_loss: 40.3611\n",
      "Epoch 174/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 28.8330 - val_loss: 42.5173\n",
      "Epoch 175/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 32.7647 - val_loss: 47.6756\n",
      "Epoch 176/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 33.4856 - val_loss: 42.1589\n",
      "Epoch 177/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 44.1326 - val_loss: 41.5036\n",
      "Epoch 178/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 31.7781 - val_loss: 47.9107\n",
      "Epoch 179/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 34.2916 - val_loss: 47.0203\n",
      "Epoch 180/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 39.7132 - val_loss: 68.1792\n",
      "Epoch 181/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 42.6847 - val_loss: 65.4399\n",
      "Epoch 182/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 68.6036 - val_loss: 92.1135\n",
      "Epoch 183/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 72.6938 - val_loss: 80.6996\n",
      "Epoch 184/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 65.2617 - val_loss: 95.7433\n",
      "Epoch 185/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 95.8865 - val_loss: 63.1715\n",
      "Epoch 186/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 64.0577 - val_loss: 52.9886\n",
      "Epoch 187/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 47.5652 - val_loss: 48.4609\n",
      "Epoch 188/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 46.7177 - val_loss: 55.8576\n",
      "Epoch 189/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 45.2336 - val_loss: 46.2458\n",
      "Epoch 190/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 47.3225 - val_loss: 52.6932\n",
      "Epoch 191/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 35.7435 - val_loss: 41.1986\n",
      "Epoch 192/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 34.5870 - val_loss: 39.2930\n",
      "Epoch 193/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 37.6283 - val_loss: 37.5361\n",
      "Epoch 194/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 39.9994 - val_loss: 40.1415\n",
      "Epoch 195/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 30.0784 - val_loss: 40.2829\n",
      "Epoch 196/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 31.3966 - val_loss: 45.5942\n",
      "Epoch 197/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 40.1550 - val_loss: 42.1967\n",
      "Epoch 198/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 36.2202 - val_loss: 43.9330\n",
      "Epoch 199/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 30.8317 - val_loss: 35.7319\n",
      "Epoch 200/200\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 30.0627 - val_loss: 40.3887\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\culture\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 6.355207960706002\n",
      "Mean Absolute Error: 4.716572159797915\n",
      "R2 Score: 0.8802623832045587\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step\n",
      "Loaded Model Root Mean Squared Error: 6.355207960706002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\culture\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# 构建 LSTM 模型\n",
    "def build_lstm_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # 第一层 LSTM 层\n",
    "    x = layers.LSTM(64, return_sequences=True)(inputs)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # 第二层 LSTM 层\n",
    "    x = layers.LSTM(128, return_sequences=True)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # 第三层 LSTM 层\n",
    "    x = layers.LSTM(256)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # 全连接层\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dense(1)(x)  # 输出预测值\n",
    "\n",
    "    # 创建模型\n",
    "    model = models.Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 创建 LSTM 模型并编译\n",
    "model = build_lstm_model((X_train.shape[1], 1))\n",
    "\n",
    "# 使用较小的学习率\n",
    "optimizer = optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# 打印模型概述\n",
    "model.summary()\n",
    "\n",
    "# 训练模型，epochs 设置为 200\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 评估模型\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "\n",
    "# 保存模型\n",
    "model.save('cabbage_price_lstm_model.h5')\n",
    "\n",
    "# 加载并测试模型\n",
    "loaded_model = models.load_model('cabbage_price_lstm_model.h5')\n",
    "loaded_y_pred = loaded_model.predict(X_test)\n",
    "loaded_rmse = mean_squared_error(y_test, loaded_y_pred, squared=False)\n",
    "print(f\"Loaded Model Root Mean Squared Error: {loaded_rmse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
